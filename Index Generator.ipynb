{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input document structure (Below 'input_structure' is sample structure of input document. modify content inside of [](square brackets).)\n",
    "input_structure = \"\"\"\n",
    "[Abstract]\n",
    "1. [Introduction]\n",
    "2. [Background]\n",
    "3. [Model Architecture]\n",
    "\t3.1. [Encoder and Decoder Stacks]\n",
    "\t3.2. [Attention]\n",
    "    \t3.2.1. [Scaled Dot-Product Attention]\n",
    "        3.2.2. [Multi-Head Attention]\n",
    "        3.2.3. [Applications of Attention in our Model]\n",
    "\t3.3. [Position-wise Feed-Forward Networks]\n",
    "\t3.4. [Embeddings and Softmax]\n",
    "    3.5. [Positional Encoding]\n",
    "4. [Why Self-Attention]\n",
    "5. [Training]\n",
    "\t5.1. [Training Data and Batching]\n",
    "\t5.2. [Hardware and Schedule]\n",
    "\t5.3. [Optimizer]\n",
    "\t5.4. [Regularization]\n",
    "\t5.5. [Visualization of learned weights]\n",
    "6. [Results]\n",
    "\t6.1. [Machine Translation]\n",
    "\t6.2. [Model Variations]\n",
    "\t6.3. [English Constituency Parsing]\n",
    "7. [Conclusion]\n",
    "[Self Q&A]\n",
    "[Opinion]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_anchor_link(section_title):\n",
    "    temp = []\n",
    "    anchor = section_title.strip().replace('    ', '\\t').split('\\n')\n",
    "    matches = re.findall(r'\\[(.*?)\\]', section_title.lower().replace(' ', '-'))\n",
    "    for i in range(len(anchor)):\n",
    "        result = f\"{anchor[i]}(#{matches[i]})\"\n",
    "        temp.append(result)\n",
    "        anchor_link = '\\n'.join(temp)\n",
    "    return anchor_link\n",
    "\n",
    "def generate_markdown_headers(anchor_link):\n",
    "    temp = []\n",
    "    anchor = anchor_link.split('\\n')\n",
    "    anchor_filter = [re.sub(r'\\([^)]*\\)', '', x) for x in anchor]\n",
    "    replace_ = ['##'+x.replace('\\t','#').replace('[','').replace(']','') for x in anchor_filter]\n",
    "    matches = re.findall(r'\\[(.*?)\\]', anchor_link.lower().replace(' ', '-'))\n",
    "    for i in range(len(anchor)):\n",
    "        index = replace_[i].rindex('#')\n",
    "        replace_[i] = replace_[i][:index+1] + ' ' + replace_[i][index+1:]\n",
    "        markdown_text = replace_[i]\n",
    "        tag = f'<a name=\"{matches[i]}\"></a>'\n",
    "        temp.append(markdown_text + ' ' + tag)\n",
    "        markdown_text_final = '\\n'.join(temp)\n",
    "    return markdown_text_final\n",
    "\n",
    "anchor_link = generate_anchor_link(input_structure)\n",
    "markdown_text_final = generate_markdown_headers(anchor_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Abstract](#abstract)\n",
      "1. [Introduction](#introduction)\n",
      "2. [Background](#background)\n",
      "3. [Model Architecture](#model-architecture)\n",
      "\t3.1. [Encoder and Decoder Stacks](#encoder-and-decoder-stacks)\n",
      "\t3.2. [Attention](#attention)\n",
      "\t\t3.2.1. [Scaled Dot-Product Attention](#scaled-dot-product-attention)\n",
      "\t\t3.2.2. [Multi-Head Attention](#multi-head-attention)\n",
      "\t\t3.2.3. [Applications of Attention in our Model](#applications-of-attention-in-our-model)\n",
      "\t3.3. [Position-wise Feed-Forward Networks](#position-wise-feed-forward-networks)\n",
      "\t3.4. [Embeddings and Softmax](#embeddings-and-softmax)\n",
      "\t3.5. [Positional Encoding](#positional-encoding)\n",
      "4. [Why Self-Attention](#why-self-attention)\n",
      "5. [Training](#training)\n",
      "\t5.1. [Training Data and Batching](#training-data-and-batching)\n",
      "\t5.2. [Hardware and Schedule](#hardware-and-schedule)\n",
      "\t5.3. [Optimizer](#optimizer)\n",
      "\t5.4. [Regularization](#regularization)\n",
      "\t5.5. [Visualization of learned weights](#visualization-of-learned-weights)\n",
      "6. [Results](#results)\n",
      "\t6.1. [Machine Translation](#machine-translation)\n",
      "\t6.2. [Model Variations](#model-variations)\n",
      "\t6.3. [English Constituency Parsing](#english-constituency-parsing)\n",
      "7. [Conclusion](#conclusion)\n",
      "[Self Q&A](#self-q&a)\n",
      "[Opinion](#opinion)\n"
     ]
    }
   ],
   "source": [
    "print(anchor_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Abstract <a name=\"abstract\"></a>\n",
      "## 1. Introduction <a name=\"introduction\"></a>\n",
      "## 2. Background <a name=\"background\"></a>\n",
      "## 3. Model Architecture <a name=\"model-architecture\"></a>\n",
      "### 3.1. Encoder and Decoder Stacks <a name=\"encoder-and-decoder-stacks\"></a>\n",
      "### 3.2. Attention <a name=\"attention\"></a>\n",
      "#### 3.2.1. Scaled Dot-Product Attention <a name=\"scaled-dot-product-attention\"></a>\n",
      "#### 3.2.2. Multi-Head Attention <a name=\"multi-head-attention\"></a>\n",
      "#### 3.2.3. Applications of Attention in our Model <a name=\"applications-of-attention-in-our-model\"></a>\n",
      "### 3.3. Position-wise Feed-Forward Networks <a name=\"position-wise-feed-forward-networks\"></a>\n",
      "### 3.4. Embeddings and Softmax <a name=\"embeddings-and-softmax\"></a>\n",
      "### 3.5. Positional Encoding <a name=\"positional-encoding\"></a>\n",
      "## 4. Why Self-Attention <a name=\"why-self-attention\"></a>\n",
      "## 5. Training <a name=\"training\"></a>\n",
      "### 5.1. Training Data and Batching <a name=\"training-data-and-batching\"></a>\n",
      "### 5.2. Hardware and Schedule <a name=\"hardware-and-schedule\"></a>\n",
      "### 5.3. Optimizer <a name=\"optimizer\"></a>\n",
      "### 5.4. Regularization <a name=\"regularization\"></a>\n",
      "### 5.5. Visualization of learned weights <a name=\"visualization-of-learned-weights\"></a>\n",
      "## 6. Results <a name=\"results\"></a>\n",
      "### 6.1. Machine Translation <a name=\"machine-translation\"></a>\n",
      "### 6.2. Model Variations <a name=\"model-variations\"></a>\n",
      "### 6.3. English Constituency Parsing <a name=\"english-constituency-parsing\"></a>\n",
      "## 7. Conclusion <a name=\"conclusion\"></a>\n",
      "## Self Q&A <a name=\"self-q&a\"></a>\n",
      "## Opinion <a name=\"opinion\"></a>\n"
     ]
    }
   ],
   "source": [
    "print(markdown_text_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
