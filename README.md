# Paper_study
This page is an archive of the Deep Learning Paper Reading study.
 
| Title | Field | Link | Performance Task | Review | publication |
|:---------------:|:-------------:|:-------------:|:-------------:|-------------:|:-------------:|
| Word2Vec(Efficient Estimation of Word Representations in Vector Space) | NLP | [Paper](https://arxiv.org/pdf/1301.3781.pdf) <br> [github](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py) |  [Word Similarity](https://paperswithcode.com/task/word-similarity) ([benchmark](https://paperswithcode.com/paper/efficient-estimation-of-word-representations)) | [Review](https://velog.io/@kunha98/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0Word2Vec-Efficient-Estimation-of-Word-Representations-in-Vector-Space-ICLR-2013) | ICLR 2013 |
| ELMo(Deep contextualized word representations) | NLP | [Paper](https://arxiv.org/pdf/1802.05365.pdf) <br> [github](https://github.com/allenai/allennlp/blob/main/allennlp/modules/elmo.py) | 9 NLP tasks ([benchmark](https://paperswithcode.com/paper/deep-contextualized-word-representations)) | [Review](https://velog.io/@kunha98/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0ELMo-Deep-contextualized-word-representations-NAACL-2018) | NAACL 2018 |
| Seq2Seq(Sequence to Sequence Learning with Neural Networks) | NLP | [Paper](https://arxiv.org/pdf/1409.3215.pdf) <br> [github](https://github.com/farizrahman4u/seq2seq) | [Machine Translation](https://paperswithcode.com/task/machine-translation), [Time Series Forecasting](https://paperswithcode.com/task/time-series-forecasting), [Traffic Prediction](https://paperswithcode.com/task/traffic-prediction) ([benchmark](https://paperswithcode.com/paper/sequence-to-sequence-learning-with-neural)) | [Review](https://velog.io/@kunha98/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0Seq2Seq-Sequence-to-Sequence-Learning-with-Neural-Networks-NIPS-2014) | NIPS 2014 |
| Transformer(Attention Is All You Need) | NLP | [Paper](https://arxiv.org/pdf/1706.03762.pdf) <br> [github](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py) | 10 NLP tasks ([benchmark](https://paperswithcode.com/paper/attention-is-all-you-need)) | [Review](https://velog.io/@kunha98/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0Transformer-Attention-Is-All-You-Need-NIPS-2017) | NIPS 2017 |
| GPT-1(Improving Language Understanding by Generative Pre-Training) | NLP | [Paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) <br> [github](https://github.com/openai/finetune-transformer-lm) | 8 NLP tasks ([benchmark](https://paperswithcode.com/paper/improving-language-understanding-by)) | [Review](https://velog.io/@kunha98/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0GPT1-Improving-Language-Understandingby-Generative-Pre-Training-Preprint-2018) | Preprint 2018 |
| BERT(Pre-training of Deep Bidirectional Transformers for Language Understanding) | NLP | [Paper](https://arxiv.org/pdf/1810.04805v2.pdf) <br> [github](https://github.com/google-research/bert) | 18 NLP tasks ([benchmark](https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional)) | [Review](https://velog.io/@kunha98/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-NAACL-2019) | NAACL 2019 |
| CRAFT(Character Region Awareness for Text Detection) | CV | [Paper](https://arxiv.org/pdf/1904.01941.pdf) <br> [github](https://github.com/clovaai/CRAFT-pytorch) | [Scene Text Detection](https://paperswithcode.com/task/scene-text-detection) ([benchmark](https://paperswithcode.com/paper/character-region-awareness-for-text-detection)) | [Review](https://velog.io/@kunha98/Character-Region-Awareness-for-Text-Detection) | CVPR 2019 |
| GPT-2(Language Models are Unsupervised Multitask Learners) | NLP | [Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) <br> [github](https://github.com/openai/gpt-2) | 13 NLP tasks ([benchmark](https://paperswithcode.com/paper/language-models-are-unsupervised-multitask)) | [Review](https://velog.io/@kunha98/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0GPT-2-Language-Models-are-Unsupervised-Multitask-Learners-Preprint-2019) | Preprint 2019 |
| LayoutReader: Pre-training of Text and Layout for Reading Order Detection | NLP,CV | [Paper](https://aclanthology.org/2021.emnlp-main.389.pdf) <br> [github](https://github.com/microsoft/unilm/tree/master/layoutreader) |  [Document Layout Analysis](https://paperswithcode.com/task/document-layout-analysis), [Optical Character Recognition(OCR)](https://paperswithcode.com/task/optical-character-recognition) ([benchmark](https://paperswithcode.com/paper/layoutreader-pre-training-of-text-and-layout)) |  | EMNLP 2021 |